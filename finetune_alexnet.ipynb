{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "With this script you can finetune AlexNet as provided in the alexnet.py\n",
    "class on any given dataset. \n",
    "Specify the configuration settings at the beginning according to your \n",
    "problem.\n",
    "This script was written for TensorFlow 1.0 and come with a blog post \n",
    "you can find here:\n",
    "  \n",
    "https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
    "\n",
    "Author: Frederik Kratzert \n",
    "contact: f.kratzert(at)gmail.com\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from alexnet import AlexNet\n",
    "from datagenerator import ImageDataGenerator\n",
    "\n",
    "\"\"\"\n",
    "Configuration settings\n",
    "\"\"\"\n",
    "\n",
    "# Path to the textfiles for the trainings and validation set\n",
    "train_file = './train.txt'\n",
    "val_file = './val.txt'\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.5\n",
    "num_classes = 1000\n",
    "train_layers = ['fc8', 'fc7']\n",
    "\n",
    "# How often we want to write the tf.summary data to disk\n",
    "display_step = 1\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"./tmp/finetune_alexnet/test\"\n",
    "checkpoint_path = \"./tmp/finetune_alexnet/\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc7/weights:0/gradient is illegal; using fc7/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0/gradient is illegal; using fc7/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0 is illegal; using fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0 is illegal; using fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "# TF placeholder for graph input and output\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Initialize model\n",
    "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "# Link variable to model output\n",
    "score = model.fc8\n",
    "\n",
    "# List of trainable variables of the layers we want to train\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = y))  \n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "  # Get gradients of all trainable variables\n",
    "  gradients = tf.gradients(loss, var_list)\n",
    "  gradients = list(zip(gradients, var_list))\n",
    "  \n",
    "  # Create optimizer and apply gradient descent to the trainable variables\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# Add gradients to summary  \n",
    "for gradient, var in gradients:\n",
    "  tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary  \n",
    "for var in var_list:\n",
    "  tf.summary.histogram(var.name, var)\n",
    "  \n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "  \n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "  correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  \n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-12 17:58:46.367272 Start training...\n",
      "2017-07-12 17:58:46.367506 Open Tensorboard at --logdir ./tmp/finetune_alexnet/test\n",
      "2017-07-12 17:58:46.367687 Epoch number: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initalize the data generator seperately for the training and validation set\n",
    "train_generator = ImageDataGenerator(train_file, \n",
    "                                     horizontal_flip = True, shuffle = True)\n",
    "val_generator = ImageDataGenerator(val_file, shuffle = False) \n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = np.floor(train_generator.data_size / batch_size).astype(np.int16)\n",
    "val_batches_per_epoch = np.floor(val_generator.data_size / batch_size).astype(np.int16)\n",
    "\n",
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    " \n",
    "  # Initialize all variables\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "  # Add the model graph to TensorBoard\n",
    "  writer.add_graph(sess.graph)\n",
    "  \n",
    "  # Load the pretrained weights into the non-trainable layer\n",
    "  model.load_initial_weights(sess)\n",
    "  \n",
    "  print(\"{} Start training...\".format(datetime.now()))\n",
    "  print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), \n",
    "                                                    filewriter_path))\n",
    "  \n",
    "  # Loop over number of epochs\n",
    "  for epoch in range(num_epochs):\n",
    "    \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "        \n",
    "        step = 1\n",
    "        \n",
    "        while step < train_batches_per_epoch:\n",
    "            \n",
    "            # Get a batch of images and labels\n",
    "            batch_xs, batch_ys = train_generator.next_batch(batch_size)\n",
    "            \n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: batch_xs, \n",
    "                                          y: batch_ys, \n",
    "                                          keep_prob: dropout_rate})\n",
    "            \n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step%display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: batch_xs, \n",
    "                                                        y: batch_ys, \n",
    "                                                        keep_prob: 1.})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                \n",
    "            step += 1\n",
    "            \n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            batch_tx, batch_ty = val_generator.next_batch(batch_size)\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_tx, \n",
    "                                                y: batch_ty, \n",
    "                                                keep_prob: 1.})\n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), test_acc))\n",
    "        \n",
    "        # Reset the file pointer of the image data generator\n",
    "        val_generator.reset_pointer()\n",
    "        train_generator.reset_pointer()\n",
    "        \n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))  \n",
    "        \n",
    "        #save checkpoint of the model\n",
    "        checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)  \n",
    "        \n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
